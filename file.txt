Cài đặt SUSE Highly Available NFS Storage with DRBD and Pacemaker
Created Wednesday 25 December 2024


I. CẤU HÌNH CỤM CLUSTER
Link: https://documentation.suse.com/sle-ha/15-SP6/pdf/article-installation_en.pdf

B1: Cấu hình máy chủ trên cả 2 node hoặc nhiều hơn
- Đảm bảo SUSE có sẵn license: SUSE Linux Enterprise Server 
Kiểm tra:
# SUSEConnect --status
Chưa có thì thực hiện lấy key và thực hiện:
+ SUSE Linux Enterprise Server
# SUSEConnect -r 32FC97A7A137A645 -e trieusinhkien@gmail.com
+ Kiểm tra lại 
# # SUSEConnect --status

- Cài đặt các gói cần thiết
# zypper install iputils
# zypper install vim net-tools snapd -y
# zypper install nfs-kernel-server 
# zypper install kernel-default kernel-default-extra
# zypper install cron
+ Cài đặt SUSE Linux Enterprise High Availability
# zypper in -t pattern ha_sles
+ Đăng kí license cho SUSE Linux Enterprise High Availability
+ SUSE Linux Enterprise High Availability 
# SUSEConnect -p sle-ha/15.4/x86_64 -r 5CA05653225C9F3D
+ Kiểm tra lại 
# # SUSEConnect --status

- Cấu hình host
192.168.252.100 DRBD1
192.168.252.101 DRBD2
- Cấu hình IP:
+ set ip tĩnh:
Card chính
# vim /etc/sysconfig/network/ifcfg-eth0
BOOTPROTO='static'
IPADDR='192.168.252.100/24'
GATEWAY='192.168.252.1'
STARTMODE='auto'
ZONE='public'
Tạo thêm card mạng ảo kết nối các node với nhau nếu cần
# vim /etc/sysconfig/network/ifcfg-eth1
BOOTPROTO='static'
IPADDR='192.168.122.110/24'
GATEWAY='192.168.122.1'
STARTMODE='auto'
ZONE='public'
- Tạo copy key ssh kết nối 2 node với nhau
Node 1
ssh-keygen 
ssh-copy-id root@192.168.252.100
Node 2
ssh-keygen 
ssh-copy-id root@192.168.252.101


B2.  Cấu hình SLE_HA 
Trên node1
1. Using SBD for node fencing  ( không sử dụng fencing bỏ qua bước này)
# find /lib/modules/$(uname -r) -type f -name "*.ko*" | grep watchdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
Kiểm tra
# lsmod | grep softdog
Nếu cấu hình sử dụng SBD lỗi  mở file 
# vim /etc/sysconfig/sbd
Sửa
SBD_WATCHDOG_DEV=""
Thành
SBD_WATCHDOG_DEV="/dev/watchdog"

2. Cài đặt gói cần thiết và cấu hình cluster
- Cài đặt gói cần thiết
# zypper install drbd-kmp-default
# modprobe drbd
# lsmod | grep drbd

- Start the bootstrap script:
# crm cluster init --name ha_cluster

- Đặt pass cho hacluster truy cập Hawk2
 # passwd hacluster
Truy cập địa chỉ giao diện quản trị Hawk2
https://192.168.252.100:7630

- Thiết lập ổ đĩa data 
pvcreate /dev/sda
vgcreate vg_data /dev/sda
lvcreate -L 10G -n lv_data vg_data

- Tạo file cấu hình DRBD ( thiết lập trên 1 node trước nên tạm # lại các dòng của node2)

vim /etc/drbd.d/drbd

resource drbd {
   volume 0 {
	  device           /dev/drbd0;
	  disk             /dev/vg_data/lv_data;  # LVM volume chứa dữ liệu
	  meta-disk        internal;             # Metadata được lưu trữ trong thiết bị
   }
   net {
	  protocol C;                             # Đồng bộ dữ liệu trong thời gian thực
	  cram-hmac-alg sha256;                   # Bảo mật giao tiếp giữa các node
	  shared-secret "EtLBahwwE1gOWFv";        # Khóa bảo mật dùng chung
	  after-sb-0pri discard-zero-changes;     # Xử lý split-brain khi cả hai không phải Primary
	  after-sb-1pri discard-secondary;        # Xóa dữ liệu node Secondary nếu có split-brain
	  after-sb-2pri disconnect;               # Ngắt kết nối nếu cả hai là Primary
	  verify-alg sha256;                      # Thuật toán kiểm tra tính toàn vẹn
	  allow-two-primaries no;                 # Không cho phép dual-primary
   }
   handlers {
	  fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";          # Fencing khi mất kết nối
	  after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh"; # Unfencing sau khi đồng bộ
	  split-brain "/usr/lib/drbd/notify-split-brain.sh root";  # Thông báo split-brain
   }
#   connection-mesh {
#           hosts     alice bob;                   # Kết nối mạng giữa các node
#  }
   startup {
	  wfc-timeout 300;                       # Thời gian chờ khởi động
	  degr-wfc-timeout 120;                  # Thời gian chờ trong trạng thái degraded
	  become-primary-on alice;               # Chỉ định alice là Primary mặc định
   }
   disk {
	  on-io-error detach;                    # Ngắt kết nối khi gặp lỗi I/O
   }
   on alice {
	  address   192.168.252.100:7790;        # Địa chỉ IP và cổng cho node alice
	  node-id   0;                           # Node ID của alice
   }
#   on bob {
#         address   192.168.252.101:7790;        # Địa chỉ IP và cổng cho node bob
#         node-id   1;                           # Node ID của bob
# }
}

- Thiết lập cấu hình DRBD
# drbdadm create-md drbd
# drbdadm up drbd
# drbdadm primary --force drbd
# mkfs.ext4 /dev/drbd0
# drbdadm status

- Thêm cấu hình crm cluster

crm configure edit

node 1: alice
# Tài nguyên IP ảo
primitive admin-ip IPaddr2 \
		params ip=192.168.122.120 \
		op monitor interval=10 timeout=20
# Tài nguyên DRBD
primitive drbd ocf:linbit:drbd \
		params drbd_resource="drbd" \
		op start interval=0 timeout=240s \
		op stop interval=0 timeout=100s \
		op monitor interval=20 role=Master timeout=30s \
		op monitor interval=30 role=Slave timeout=30s
# Clone DRBD (cho phép DRBD hoạt động trên cả hai node nhưng chỉ một node ở trạng thái Primary)
clone cl-drbd drbd \
		meta promotable=true \
		promoted-max=1 \
		promoted-node-max=1 \
		clone-max=2 \
		clone-node-max=1 \
		notify=true \
		interleave=true
# Tài nguyên Filesystem (mount /dev/drbd0 lên /mnt)
primitive fs-drbd Filesystem \
		params device="/dev/drbd0" directory="/mnt" fstype="ext4" \
		op start interval=0 timeout=60s \
		op stop interval=0 timeout=60s \
		op monitor interval=20s timeout=40s
# Thuộc tính cụm
property cib-bootstrap-options: \
		have-watchdog=false \
		dc-version="2.1.2+20211124.ada5c3b36-150400.4.20.1-2.1.2+20211124.ada5c3b36" \
		cluster-infrastructure=corosync \
		cluster-name=ha_cluster \
		stonith-enabled=false
# Mặc định tài nguyên
rsc_defaults build-resource-defaults: \
		resource-stickiness=1 \
		migration-threshold=3
# Mặc định các hoạt động
op_defaults op-options: \
		timeout=600 \
		record-pending=true
# Quan hệ ưu tiên và thứ tự
# DRBD phải được Promoted trước khi mount Filesystem
colocation col-fs-with-drbd inf: fs-drbd cl-drbd:Master
order o-drbd-before-fs Mandatory: cl-drbd:promote fs-drbd:start
# DRBD phải Promoted trước khi sử dụng IP
colocation col-admin-with-drbd inf: admin-ip cl-drbd:Master
order o-drbd-before-admin Mandatory: cl-drbd:promote +admin-ip:start


- Đoạn cấu hình đặt alice luôn là Master 

node 1: alice

# Tài nguyên IP ảo
primitive admin-ip IPaddr2 \
	params ip=192.168.122.120 \
	op monitor interval=10 timeout=20

# Tài nguyên DRBD
primitive drbd ocf:linbit:drbd \
	params drbd_resource="drbd" \
	op start interval=0 timeout=240s \
	op stop interval=0 timeout=100s \
	op monitor interval=20 role=Master timeout=30s \
	op monitor interval=30 role=Slave timeout=30s

# Clone DRBD (cho phép DRBD hoạt động trên cả hai node nhưng chỉ một node ở trạng thái Primary)
clone cl-drbd drbd \
	meta promotable=true \
	promoted-max=1 \
	promoted-node-max=1 \
	clone-max=2 \
	clone-node-max=1 \
	notify=true \
	interleave=true

# Tài nguyên Filesystem (mount /dev/drbd0 lên /mnt)
primitive fs-drbd Filesystem \
	params device="/dev/drbd0" directory="/mnt" fstype="ext4" \
	op start interval=0 timeout=60s \
	op stop interval=0 timeout=60s \
	op monitor interval=20s timeout=40s

# Thuộc tính cụm
property cib-bootstrap-options: \
	have-watchdog=false \
	dc-version="2.1.2+20211124.ada5c3b36-150400.4.20.1-2.1.2+20211124.ada5c3b36" \
	cluster-infrastructure=corosync \
	cluster-name=ha_cluster \
	stonith-enabled=false \
	symmetric-cluster=false \
	default-resource-stickiness=100

# Mặc định tài nguyên
rsc_defaults build-resource-defaults: \
	resource-stickiness=1 \
	migration-threshold=3

# Mặc định các hoạt động
op_defaults op-options: \
	timeout=600 \
	record-pending=true

# Quan hệ ưu tiên và thứ tự
# DRBD phải được Promoted trước khi mount Filesystem
colocation col-fs-with-drbd inf: fs-drbd cl-drbd:Master
order o-drbd-before-fs Mandatory: cl-drbd:promote fs-drbd:start

# DRBD phải Promoted trước khi sử dụng IP
colocation col-admin-with-drbd inf: admin-ip cl-drbd:Master
order o-drbd-before-admin Mandatory: cl-drbd:promote admin-ip:start

# Đảm bảo alice luôn là Master
location loc-alice-pref cl-drbd:Master 200: alice
location loc-bob-pref cl-drbd:Master -100: bob

# Đồng bộ trước khi Promoted
order sync-then-promote Mandatory: cl-drbd:promote fs-drbd:start

# Chỉ định master-max và target-role
meta cl-drbd master-max=1 master-node-max=1 target-role=Master


Lưu lại và kiểm tra 
# crm configure commit
# crm status
# drbdadm status
Đảm bảo ổ đã được mount lên /mnt ghi dữ liệu và kiểm tra



Trên node2
1. Using SBD for node fencing  ( không sử dụng fencing bỏ qua bước này)
# find /lib/modules/$(uname -r) -type f -name "*.ko*" | grep watchdog
# echo softdog > /etc/modules-load.d/watchdog.conf
# systemctl restart systemd-modules-load
Kiểm tra
# lsmod | grep softdog
Nếu cấu hình sử dụng SBD lỗi  mở file 
# vim /etc/sysconfig/sbd
Sửa
SBD_WATCHDOG_DEV=""
Thành
SBD_WATCHDOG_DEV="/dev/watchdog"

2. Cài đặt gói cần thiết và cấu hình cluster
- Cài đặt gói cần thiết
# zypper install drbd-kmp-default
# modprobe drbd
# lsmod | grep drbd

- Thiết lập ổ đĩa data 
pvcreate /dev/sda
vgcreate vg_data /dev/sda
lvcreate -L 10G -n lv_data vg_data

- Thiết lập cấu hình DRBD
# drbdadm create-md drbd
# drbdadm up drbd
# drbdadm status

- Đặt pass cho hacluster truy cập Hawk2
 # passwd hacluster

- Join Node2
+ mở file /etc/drbb.d/drbd.res 
   Bỏ # các dòng liên quan đến kết nối node2
+ Sync cấu hình /etc/drbb.d/drbd.res từ node1 sang node 2
# rsync -av  /etc/drbb.d/drbd.res root@192.168.252.101:/etc/drbd.d/
+ Thực hiện Join vào cụm cluster
# crm cluster join
Nhập IP node1
IP address or hostname of existing node (e.g.: 192.168.252.100) []192.168.252.100

- Kiểm tra trạng thái sau khi join trên cả 2 node
+ Trạng thái cluster
# crm status
bob:~ # crm status
Cluster Summary:
  * Stack: corosync
  * Current DC: alice (version 2.1.2+20211124.ada5c3b36-150400.4.20.1-2.1.2+20211124.ada5c3b36) - partition with quorum
  * Last updated: Mon Jan  6 15:07:57 2025
  * Last change:  Mon Jan  6 14:56:42 2025 by hacluster via crmd on alice
  * 2 nodes configured
  * 4 resource instances configured

Node List:
  * Online: [ alice bob ]

Full List of Resources:
  * admin-ip	(ocf::heartbeat:IPaddr2):	 Started alice
  * Clone Set: cl-drbd [drbd] (promotable):
	* Masters: [ alice ]
	* Slaves: [ bob ]
  * fs-drbd	(ocf::heartbeat:Filesystem):	 Started alice

-  Trạng thái DRBD
# drbdadm status 
 

alice:~ # drbdadm status
drbd role:Primary
  disk:UpToDate
  bob role:Secondary
	peer-disk:UpToDate

bob:~ # drbdadm status
drbd role:Secondary
  disk:UpToDate
  alice role:Primary
	peer-disk:UpToDat




II. CONFIG HIGHLY AVAILABLE NFS STORAGE WITH DRBD AND PACEMAKER
Link: https://documentation.suse.com/sle-ha/12-SP5/single-html/SLE-HA-nfs-quick/index.html

B1. Preparing a Two-node Cluster
# vim /etc/systemd/system/nfs-server.service.d/scope.conf

[Service]
ExecStart=
ExecStart=/usr/sbin/rpc.nfsd --scope SUSE $RPCNFSDARGS

# systemctl daemon-reload

B2. Creating LVM Devices
# crm cluster run "pvcreate /dev/sda"
# crm cluster run "vgcreate nfs /dev/sda"
Tạo phân vùng lưu dữ liệu
# crm cluster run "lvcreate -n share -L 20G nfs"
Tạo phân vùng lưu trạng thái
# crm cluster run "lvcreate -n state -L 8G nfs"
# crm cluster run "vgchange -ay nfs"

 B3. Creating DRBD devices 

3.1 
# vim /etc/drbd.d/nfs.res

resource nfs {
   volume 0 { 
	  device           /dev/drbd0; 
	  disk             /dev/nfs/state; 
	  meta-disk        internal; 
   }
   volume 1 {
	  device           /dev/drbd1;
	  disk             /dev/nfs/share;
	  meta-disk        internal;
   }

   net {
	  protocol C; 
	  fencing resource-and-stonith; 
   }

   handlers { 
	  fence-peer "/usr/lib/drbd/crm-fence-peer.9.sh";
	  after-resync-target "/usr/lib/drbd/crm-unfence-peer.9.sh";
   }

   connection-mesh { 
	  hosts     alice bob;
   }
   on alice { 
	  address   192.168.252.100:7790;
	  node-id   0;
   }
   on bob {
	  address   192.168.252.101:7790;
	  node-id   1;
   }
}

# Sync file other nodes

3.1 Activating the DRBD devices
# crm cluster run "drbdadm create-md nfs"
# crm cluster run "drbdadm up nfs"
# 
